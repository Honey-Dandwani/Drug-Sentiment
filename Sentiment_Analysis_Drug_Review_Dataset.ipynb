{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Link : https://www.kaggle.com/datasets/jessicali9530/kuc-hackathon-winter-2018"
      ],
      "metadata": {
        "id": "hhVBakFDHoXq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eww1aLWVSBs_",
        "outputId": "1991b2e4-bad5-4d37-8a29-45c1554939bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2022.5.18.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "yfRC4zhHSoC3",
        "outputId": "2c5c401f-d65a-43bd-9a82-8f7106011391"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5993e921-04ac-4048-b429-399103ed4579\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5993e921-04ac-4048-b429-399103ed4579\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"prajaktaghumatkar99\",\"key\":\"92322a3a568aebb0499d100a0ca14275\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KIlAi-USrOO"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEi94ixoJu2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df6afe67-5e51-460a-a2f0-939d8b871b27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading kuc-hackathon-winter-2018.zip to /content\n",
            " 61% 25.0M/40.7M [00:00<00:00, 47.0MB/s]\n",
            "100% 40.7M/40.7M [00:00<00:00, 70.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d jessicali9530/kuc-hackathon-winter-2018"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g26B8iskJvHH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c869b6fb-7847-4c8d-de08-dc0bbe2907df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ],
      "source": [
        "from zipfile import ZipFile\n",
        "file_name=\"kuc-hackathon-winter-2018.zip\"\n",
        "\n",
        "with ZipFile(file_name,'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHhhFxUCSu3O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d721332-cf51-48da-84aa-135b8643cbc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.0.6-cp37-none-manylinux1_x86_64.whl (76.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 76.6 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (5.5.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.21.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.3.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2022.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (1.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->catboost) (4.2.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost) (8.0.1)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.0.6\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from wordcloud import WordCloud\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import warnings; warnings.simplefilter('ignore')\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from string import punctuation\n",
        "from nltk import ngrams\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMModel,LGBMClassifier, plot_importance\n",
        "\n",
        "!pip install catboost\n",
        "from catboost import CatBoostClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3p7FxK4TymE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "5642cec2-1b87-43fc-978d-9ecad7150162"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        uniqueID                  drugName                     condition  \\\n",
              "0         206461                 Valsartan  Left Ventricular Dysfunction   \n",
              "1          95260                Guanfacine                          ADHD   \n",
              "2          92703                    Lybrel                 Birth Control   \n",
              "3         138000                Ortho Evra                 Birth Control   \n",
              "4          35696  Buprenorphine / naloxone             Opiate Dependence   \n",
              "...          ...                       ...                           ...   \n",
              "161292    191035                   Campral            Alcohol Dependence   \n",
              "161293    127085            Metoclopramide               Nausea/Vomiting   \n",
              "161294    187382                   Orencia          Rheumatoid Arthritis   \n",
              "161295     47128        Thyroid desiccated           Underactive Thyroid   \n",
              "161296    215220              Lubiprostone         Constipation, Chronic   \n",
              "\n",
              "                                                   review  rating       date  \\\n",
              "0       \"It has no side effect, I take it in combinati...       9  20-May-12   \n",
              "1       \"My son is halfway through his fourth week of ...       8  27-Apr-10   \n",
              "2       \"I used to take another oral contraceptive, wh...       5  14-Dec-09   \n",
              "3       \"This is my first time using any form of birth...       8   3-Nov-15   \n",
              "4       \"Suboxone has completely turned my life around...       9  27-Nov-16   \n",
              "...                                                   ...     ...        ...   \n",
              "161292  \"I wrote my first report in Mid-October of 201...      10  31-May-15   \n",
              "161293  \"I was given this in IV before surgey. I immed...       1   1-Nov-11   \n",
              "161294  \"Limited improvement after 4 months, developed...       2  15-Mar-14   \n",
              "161295  \"I&#039;ve been on thyroid medication 49 years...      10  19-Sep-15   \n",
              "161296  \"I&#039;ve had chronic constipation all my adu...       9  13-Dec-14   \n",
              "\n",
              "        usefulCount  \n",
              "0                27  \n",
              "1               192  \n",
              "2                17  \n",
              "3                10  \n",
              "4                37  \n",
              "...             ...  \n",
              "161292          125  \n",
              "161293           34  \n",
              "161294           35  \n",
              "161295           79  \n",
              "161296          116  \n",
              "\n",
              "[161297 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-91e96034-f155-4439-8b06-9099e242e7dd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uniqueID</th>\n",
              "      <th>drugName</th>\n",
              "      <th>condition</th>\n",
              "      <th>review</th>\n",
              "      <th>rating</th>\n",
              "      <th>date</th>\n",
              "      <th>usefulCount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>206461</td>\n",
              "      <td>Valsartan</td>\n",
              "      <td>Left Ventricular Dysfunction</td>\n",
              "      <td>\"It has no side effect, I take it in combinati...</td>\n",
              "      <td>9</td>\n",
              "      <td>20-May-12</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>95260</td>\n",
              "      <td>Guanfacine</td>\n",
              "      <td>ADHD</td>\n",
              "      <td>\"My son is halfway through his fourth week of ...</td>\n",
              "      <td>8</td>\n",
              "      <td>27-Apr-10</td>\n",
              "      <td>192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>92703</td>\n",
              "      <td>Lybrel</td>\n",
              "      <td>Birth Control</td>\n",
              "      <td>\"I used to take another oral contraceptive, wh...</td>\n",
              "      <td>5</td>\n",
              "      <td>14-Dec-09</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>138000</td>\n",
              "      <td>Ortho Evra</td>\n",
              "      <td>Birth Control</td>\n",
              "      <td>\"This is my first time using any form of birth...</td>\n",
              "      <td>8</td>\n",
              "      <td>3-Nov-15</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>35696</td>\n",
              "      <td>Buprenorphine / naloxone</td>\n",
              "      <td>Opiate Dependence</td>\n",
              "      <td>\"Suboxone has completely turned my life around...</td>\n",
              "      <td>9</td>\n",
              "      <td>27-Nov-16</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161292</th>\n",
              "      <td>191035</td>\n",
              "      <td>Campral</td>\n",
              "      <td>Alcohol Dependence</td>\n",
              "      <td>\"I wrote my first report in Mid-October of 201...</td>\n",
              "      <td>10</td>\n",
              "      <td>31-May-15</td>\n",
              "      <td>125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161293</th>\n",
              "      <td>127085</td>\n",
              "      <td>Metoclopramide</td>\n",
              "      <td>Nausea/Vomiting</td>\n",
              "      <td>\"I was given this in IV before surgey. I immed...</td>\n",
              "      <td>1</td>\n",
              "      <td>1-Nov-11</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161294</th>\n",
              "      <td>187382</td>\n",
              "      <td>Orencia</td>\n",
              "      <td>Rheumatoid Arthritis</td>\n",
              "      <td>\"Limited improvement after 4 months, developed...</td>\n",
              "      <td>2</td>\n",
              "      <td>15-Mar-14</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161295</th>\n",
              "      <td>47128</td>\n",
              "      <td>Thyroid desiccated</td>\n",
              "      <td>Underactive Thyroid</td>\n",
              "      <td>\"I&amp;#039;ve been on thyroid medication 49 years...</td>\n",
              "      <td>10</td>\n",
              "      <td>19-Sep-15</td>\n",
              "      <td>79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161296</th>\n",
              "      <td>215220</td>\n",
              "      <td>Lubiprostone</td>\n",
              "      <td>Constipation, Chronic</td>\n",
              "      <td>\"I&amp;#039;ve had chronic constipation all my adu...</td>\n",
              "      <td>9</td>\n",
              "      <td>13-Dec-14</td>\n",
              "      <td>116</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>161297 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-91e96034-f155-4439-8b06-9099e242e7dd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-91e96034-f155-4439-8b06-9099e242e7dd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-91e96034-f155-4439-8b06-9099e242e7dd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "train = pd.read_csv('drugsComTrain_raw.csv')\n",
        "test = pd.read_csv('drugsComTest_raw.csv')\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FDiSG-EKoKM"
      },
      "outputs": [],
      "source": [
        "# as both the dataset contains same columns we can combine them for better analysis\n",
        "\n",
        "data = pd.concat([train, test])\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-r7uhB8KoOC"
      },
      "outputs": [],
      "source": [
        "# describing the data\n",
        "\n",
        "data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqLCtFSXKoRk"
      },
      "outputs": [],
      "source": [
        "# taking out information from the data\n",
        "\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeSQHSCjKyNy"
      },
      "outputs": [],
      "source": [
        "# Dropping the data points with null values \n",
        "data = data.dropna(how = 'any', axis = 0)\n",
        "# lowercasing the column names so it will be easier for access ^^\n",
        "data.columns = data.columns.str.lower()\n",
        "# Sorting the dataframe\n",
        "data.sort_values(['rating'], ascending = True, inplace = True)\n",
        "data.reset_index(drop = True, inplace = True)\n",
        "# Converting the date in to date time format \n",
        "data['date'] = pd.to_datetime(data['date'])\n",
        "data.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxX1zxn6KybW"
      },
      "outputs": [],
      "source": [
        "# Total unique conditions in the dataset\n",
        "print (data['condition'].nunique(), \"\\n\")\n",
        "\n",
        "print (\"some of the conditions are : \", data['condition'].unique()[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGt1pViKKyff"
      },
      "outputs": [],
      "source": [
        "#top 10 drugs with rating equals 10\n",
        "data.loc[data['rating'] == 10, :]['drugname'].value_counts().head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgDfTYHELgYp"
      },
      "outputs": [],
      "source": [
        "# Word cloud of the reviews with rating equal to 10\n",
        "#stopwords = set(STOPWORDS)\n",
        "\n",
        "df_rate_ten = data.loc[data.rating == 10, 'review']\n",
        "text = (' '.join(df_rate_ten))\n",
        "\n",
        "wordcloud = WordCloud(width = 1000, height = 500, background_color = 'white').generate(text)\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTokTf2PLxLG"
      },
      "outputs": [],
      "source": [
        "# This barplot shows the top 20 drugs with the 10/10 rating\n",
        "\n",
        "# Setting the Parameter\n",
        "sns.set(font_scale = 1.2, style = 'darkgrid')\n",
        "plt.rcParams['figure.figsize'] = [15, 8]\n",
        "\n",
        "rating = dict(data.loc[data.rating == 10, \"drugname\"].value_counts())\n",
        "drugname = list(rating.keys())\n",
        "drug_rating = list(rating.values())\n",
        "\n",
        "sns_rating = sns.barplot(x = drugname[0:20], y = drug_rating[0:20])\n",
        "\n",
        "sns_rating.set_title('Top 20 drugs with 10/10 rating')\n",
        "sns_rating.set_ylabel(\"Number of Ratings\")\n",
        "sns_rating.set_xlabel(\"Drug Names\")\n",
        "plt.setp(sns_rating.get_xticklabels(), rotation=90);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9g7HD3hBLxNo"
      },
      "outputs": [],
      "source": [
        "# This barplot show the top 10 conditions the people are suffering.\n",
        "cond = dict(data['condition'].value_counts())\n",
        "top_condition = list(cond.keys())[0:10]\n",
        "values = list(cond.values())[0:10]\n",
        "sns.set(style = 'darkgrid', font_scale = 1.3)\n",
        "plt.rcParams['figure.figsize'] = [18, 7]\n",
        "\n",
        "sns_ = sns.barplot(x = top_condition, y = values, palette = 'winter')\n",
        "sns_.set_title(\"Top 10 conditions\")\n",
        "sns_.set_xlabel(\"Conditions\")\n",
        "sns_.set_ylabel(\"Count\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElAr7iYjLxSo"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# A countplot of the ratings so we can see the distribution of the ratings\n",
        "plt.rcParams['figure.figsize'] = [20,8]\n",
        "sns.set(font_scale = 1.4, style = 'darkgrid')\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "\n",
        "sns_1 = sns.countplot(data['rating'], palette = 'spring', order = list(range(10, 0, -1)), ax = ax[0])\n",
        "sns_2 = sns.distplot(data['rating'], ax = ax[1])\n",
        "sns_1.set_title('Count of Ratings')\n",
        "sns_1.set_xlabel(\"Rating\")\n",
        "\n",
        "sns_2.set_title('Distribution of Ratings')\n",
        "sns_2.set_xlabel(\"Rating\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAQSL1Z-GpKq"
      },
      "source": [
        "**Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoqCSkBnGnRL"
      },
      "outputs": [],
      "source": [
        "# Giving the Sentiment according to the ratings\n",
        "import math\n",
        "data['sentiment_rate'] = data['rating'].apply(lambda x: math.ceil(x/2))\n",
        "data['sentiment_rate'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyQm1fKhQvGU"
      },
      "source": [
        "Word count plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBbt71_SMUS5"
      },
      "outputs": [],
      "source": [
        "def review_clean(review): \n",
        "    # changing review to lower cases.\n",
        "    lower = review.str.lower()\n",
        "    \n",
        "    # Replacing the repeating pattern of &#039;\n",
        "    pattern_remove = lower.str.replace(\"&#039;\", \"\")\n",
        "    \n",
        "    # Removing all the special Characters\n",
        "    special_remove = pattern_remove.str.replace(r'[^\\w\\d\\s]',' ')\n",
        "    \n",
        "    # Removing all the non ASCII characters\n",
        "    ascii_remove = special_remove.str.replace(r'[^\\x00-\\x7F]+',' ')\n",
        "    \n",
        "    # Removing the leading and trailing Whitespaces\n",
        "    whitespace_remove = ascii_remove.str.replace(r'^\\s+|\\s+?$','')\n",
        "    \n",
        "    # Replacing multiple Spaces with Single Space\n",
        "    multiw_remove = whitespace_remove.str.replace(r'\\s+',' ')\n",
        "    \n",
        "    # Replacing Two or more dots with one\n",
        "    dataframe = multiw_remove.str.replace(r'\\.{2,}', ' ')\n",
        "    \n",
        "    return dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjyYeykPSvSb"
      },
      "outputs": [],
      "source": [
        "# Contraction Dictionary for the expansion\n",
        "\n",
        "contractions_dict = {\n",
        "    \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\",\n",
        "    \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\",\n",
        "    \"doesn’t\": \"does not\", \"don't\": \"do not\", \"don’t\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\", \"hasn't\": \"has not\",\n",
        "    \"haven't\": \"have not\", \"he'd\": \"he had\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\",\n",
        "    \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
        "    \"i'll\": \"i will\", \"i'll've\": \"i will have\", \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\",\n",
        "    \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\"might've\": \"might have\",\n",
        "    \"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\",\n",
        "    \"needn't\": \"need not\", \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\",\n",
        "    \"shan't\": \"shall not\",\"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\",\n",
        "    \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
        "    \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so is\", \"that'd\": \"that would\", \"that'd've\": \"that would have\",\n",
        "    \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"they'd\": \"they would\",\n",
        "    \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
        "    \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n",
        "    \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "    \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "    \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\",\n",
        "    \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\",\n",
        "    \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y’all\": \"you all\", \"y'all'd\": \"you all would\",\n",
        "    \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\",\n",
        "    \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", \"ain’t\": \"am not\", \"aren’t\": \"are not\",\n",
        "    \"can’t\": \"cannot\", \"can’t’ve\": \"cannot have\", \"’cause\": \"because\", \"could’ve\": \"could have\", \"couldn’t\": \"could not\", \"couldn’t’ve\": \"could not have\",\n",
        "    \"didn’t\": \"did not\", \"doesn’t\": \"does not\", \"don’t\": \"do not\", \"don’t\": \"do not\", \"hadn’t\": \"had not\", \"hadn’t’ve\": \"had not have\",\n",
        "    \"hasn’t\": \"has not\", \"haven’t\": \"have not\", \"he’d\": \"he had\", \"he’d’ve\": \"he would have\", \"he’ll\": \"he will\", \"he’ll’ve\": \"he will have\",\n",
        "    \"he’s\": \"he is\", \"how’d\": \"how did\", \"how’d’y\": \"how do you\", \"how’ll\": \"how will\", \"how’s\": \"how is\", \"i’d\": \"i would\", \"i’d’ve\": \"i would have\",\n",
        "    \"i’ll\": \"i will\", \"i’ll’ve\": \"i will have\", \"i’m\": \"i am\", \"i’ve\": \"i have\", \"isn’t\": \"is not\", \"it’d\": \"it would\", \"it’d’ve\": \"it would have\",\n",
        "    \"it’ll\": \"it will\", \"it’ll’ve\": \"it will have\", \"it’s\": \"it is\", \"let’s\": \"let us\", \"ma’am\": \"madam\", \"mayn’t\": \"may not\",\n",
        "    \"might’ve\": \"might have\", \"mightn’t\": \"might not\", \"mightn’t’ve\": \"might not have\", \"must’ve\": \"must have\", \"mustn’t\": \"must not\",\n",
        "    \"mustn’t’ve\": \"must not have\", \"needn’t\": \"need not\", \"needn’t’ve\": \"need not have\", \"o’clock\": \"of the clock\",\n",
        "    \"oughtn’t\": \"ought not\", \"oughtn’t’ve\": \"ought not have\", \"shan’t\": \"shall not\", \"sha’n’t\": \"shall not\", \"shan’t’ve\": \"shall not have\",\n",
        "    \"she’d\": \"she would\", \"she’d’ve\": \"she would have\", \"she’ll\": \"she will\", \"she’ll’ve\": \"she will have\", \"she’s\": \"she is\",\n",
        "    \"should’ve\": \"should have\", \"shouldn’t\": \"should not\", \"shouldn’t’ve\": \"should not have\", \"so’ve\": \"so have\", \"so’s\": \"so is\",\n",
        "    \"that’d\": \"that would\", \"that’d’ve\": \"that would have\", \"that’s\": \"that is\", \"there’d\": \"there would\", \"there’d’ve\": \"there would have\",\n",
        "    \"there’s\": \"there is\", \"they’d\": \"they would\", \"they’d’ve\": \"they would have\", \"they’ll\": \"they will\", \"they’ll’ve\": \"they will have\",\n",
        "    \"they’re\": \"they are\", \"they’ve\": \"they have\", \"to’ve\": \"to have\", \"wasn’t\": \"was not\", \"we’d\": \"we would\", \"we’d’ve\": \"we would have\",\n",
        "    \"we’ll\": \"we will\", \"we’ll’ve\": \"we will have\", \"we’re\": \"we are\", \"we’ve\": \"we have\", \"weren’t\": \"were not\", \"what’ll\": \"what will\",\n",
        "    \"what’ll’ve\": \"what will have\", \"what’re\": \"what are\", \"what’s\": \"what is\", \"what’ve\": \"what have\", \"when’s\": \"when is\",\n",
        "    \"when’ve\": \"when have\", \"where’d\": \"where did\", \"where’s\": \"where is\", \"where’ve\": \"where have\", \"who’ll\": \"who will\",\n",
        "    \"who’ll’ve\": \"who will have\", \"who’s\": \"who is\", \"who’ve\": \"who have\",\"why’s\": \"why is\", \"why’ve\": \"why have\", \"will’ve\": \"will have\",\n",
        "    \"won’t\": \"will not\", \"won’t’ve\": \"will not have\", \"would’ve\": \"would have\", \"wouldn’t\": \"would not\", \"wouldn’t’ve\": \"would not have\",\n",
        "    \"y’all\": \"you all\", \"y’all\": \"you all\", \"y’all’d\": \"you all would\", \"y’all’d’ve\": \"you all would have\", \"y’all’re\": \"you all are\",\n",
        "    \"y’all’ve\": \"you all have\", \"you’d\": \"you would\", \"you’d’ve\": \"you would have\", \"you’ll\": \"you will\", \"you’ll’ve\": \"you will have\",\n",
        "    \"you’re\": \"you are\", \"you’re\": \"you are\", \"you’ve\": \"you have\"\n",
        "}\n",
        "contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
        "\n",
        "# Function expand the contractions if there's any\n",
        "def expand_contractions(s, contractions_dict=contractions_dict):\n",
        "    def replace(match):\n",
        "        return contractions_dict[match.group(0)]\n",
        "    return contractions_re.sub(replace, s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQtGnhs0SXg_"
      },
      "outputs": [],
      "source": [
        "#applying the above assumptions into a column\n",
        "data['review_clean'] = review_clean(data['review'])\n",
        "\n",
        "# Expanding the contractions\n",
        "data['review_clean'] = data['review_clean'].apply(lambda x: expand_contractions(x))\n",
        "\n",
        "# Removing punctuations\n",
        "data['review_clean'] = data['review_clean'].apply(lambda x: ''.join(word for word in x if word not in punctuation))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upxOxEk6TR4V"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6F-AgJ3sTSEB"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob\n",
        "\"\"\"I have used textblob module to give the sentiment polarity of the review. This polarity is given to both the cleaned and uncleaned review\"\"\"\n",
        "#from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import warnings; warnings.simplefilter('ignore')\n",
        "import nltk\n",
        "import string\n",
        "from nltk import ngrams\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.stem import SnowballStemmer\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Removing the stopwords from the review_clean column\n",
        "stop_words = set(stopwords.words('english'))\n",
        "data['review_clean'] = data['review_clean'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))\n",
        "\n",
        "# Removing the word stems using the Snowball Stemmer\n",
        "Snow_ball = SnowballStemmer(\"english\")\n",
        "data['review_clean'] = data['review_clean'].apply(lambda x: \" \".join(Snow_ball.stem(word) for word in x.split()))\n",
        "\n",
        "# Separating the day, month and year from the Date\n",
        "\n",
        "data['day'] = data['date'].dt.day\n",
        "data['month'] = data['date'].dt.month\n",
        "data['year'] = data['date'].dt.year\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRPGlarOTSfy"
      },
      "outputs": [],
      "source": [
        "def sentiment(review):\n",
        "    # Sentiment polarity of the reviews\n",
        "    pol = []\n",
        "    for i in review:\n",
        "        analysis = TextBlob(i)\n",
        "        pol.append(analysis.sentiment.polarity)\n",
        "    return pol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9VFz-XVYvEB"
      },
      "outputs": [],
      "source": [
        "data['sentiment'] = sentiment(data['review'])\n",
        "\n",
        "data['sentiment_clean'] = sentiment(data['review_clean'])\n",
        "\n",
        "np.corrcoef(data['sentiment'], data['rating'])\n",
        "\n",
        "np.corrcoef(data['sentiment_clean'], data['rating'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEwdd_Z5_AFy"
      },
      "source": [
        "Now we can see that the correlation coefficient between setiment of the uncleaned review included with rating is higher than with the cleaned review, so for better results lets try the cleaning without removing the stopwords and the snowball stemmer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7Vx8LAe_Rcx"
      },
      "outputs": [],
      "source": [
        "# Cleaning the reviews without removing the stop words and using snowball stemmer\n",
        "\n",
        "data['review_clean_ss'] = review_clean(data['review'])\n",
        "\n",
        "data['review_clean_ss'] = data['review_clean_ss'].apply(lambda x: expand_contractions(x))\n",
        "\n",
        "data['review_clean_ss'] = data['review_clean_ss'].apply(lambda x: ''.join(word for word in x if word not in punctuation))\n",
        "\n",
        "data['sentiment_clean_ss'] = sentiment(data['review_clean_ss'])\n",
        "\n",
        "np.corrcoef(data['sentiment_clean_ss'], data['rating'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdDwlJo2YvT2"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usT4lP6kYvXg"
      },
      "outputs": [],
      "source": [
        "# Calculating weighted ratings\n",
        "# Formula : (R*v + C*m)/(v+m)\n",
        "# R - data['rating']\n",
        "# v - data['usefulcount']\n",
        "# C (mean rating for entire dataset) - data['rating'].mean()\n",
        "# m (least # of votes/usefulcount required to be accepted in 70 percentile) - data['usefulcount'].quantile(0.70)\n",
        "\n",
        "# Calculating C\n",
        "C=int(data['rating'].mean())\n",
        "\n",
        "#Calculating m\n",
        "m = data['usefulcount'].quantile(0.70)\n",
        "m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftg7TjRtYvay"
      },
      "outputs": [],
      "source": [
        "data['weighted_rating'] = ((data['rating']*data['usefulcount']) + (C*m))/(data['usefulcount']+m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0GZmNat9Y1K"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0xF3BImkrg3"
      },
      "outputs": [],
      "source": [
        "#Word count in each review\n",
        "data['count_word']=data[\"review_clean_ss\"].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "#Unique word count \n",
        "data['count_unique_word']=data[\"review_clean_ss\"].apply(lambda x: len(set(str(x).split())))\n",
        "\n",
        "#Letter count\n",
        "data['count_letters']=data[\"review_clean_ss\"].apply(lambda x: len(str(x)))\n",
        "\n",
        "#punctuation count\n",
        "data[\"count_punctuations\"] = data[\"review\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
        "\n",
        "#upper case words count\n",
        "data[\"count_words_upper\"] = data[\"review\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
        "\n",
        "#title case words count\n",
        "data[\"count_words_title\"] = data[\"review\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
        "\n",
        "#Number of stopwords\n",
        "data[\"count_stopwords\"] = data[\"review\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]))\n",
        "\n",
        "#Average length of the words\n",
        "data[\"mean_word_len\"] = data[\"review_clean_ss\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
        "\n",
        "data['mean_word_len'] = data['mean_word_len'].fillna((data['mean_word_len'].mean()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YRD2YRokrp5"
      },
      "outputs": [],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EULTHJq5kruG"
      },
      "outputs": [],
      "source": [
        "# Correlation Heatmap of the features engineered\n",
        "plt.rcParams['figure.figsize'] = [17,15]\n",
        "sns.set(font_scale = 1.2)\n",
        "corr = data.select_dtypes(include = 'int64').corr()\n",
        "sns_ = sns.heatmap(corr, annot = True, cmap = 'YlGnBu')\n",
        "plt.setp(sns_.get_xticklabels(), rotation = 45);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeCeXTnp5Jnt"
      },
      "source": [
        "\n",
        "+Correlation Heatmap is plotted using seaborn which contains all the new features engineered and the old features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D74cZzCRksCe"
      },
      "outputs": [],
      "source": [
        "# Label Encoding Drugname and Conditions\n",
        "label_encoder_feat = {}\n",
        "for feature in ['drugname', 'condition']:\n",
        "    label_encoder_feat[feature] = LabelEncoder()\n",
        "    data[feature] = label_encoder_feat[feature].fit_transform(data[feature])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PwR9_sA5NWf"
      },
      "source": [
        "The Label Encoder is used to change the categorical values of Drug Names and the conditions in to numerical values for the machine learning modelling. There are 3,667 unique drugs in the dataset that's why One hot encoder is not used as it would generate 3,667 new features and it would be very computationally expensive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9murrJ89krM7"
      },
      "outputs": [],
      "source": [
        "#Print plot overviews of the first 5 movies.\n",
        "data['review_clean_ss'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIAJaRDZk0wm"
      },
      "outputs": [],
      "source": [
        "#Import TfIdfVectorizer from scikit-learn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#Define a TF-IDF Vectorizer Object. Remove all english stop words such as 'the', 'a'\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "#Replace NaN with an empty string\n",
        "data['review_clean_ss'] = data['review_clean_ss'].fillna('')\n",
        "\n",
        "#Construct the required TF-IDF matrix by fitting and transforming the data\n",
        "tfidf_matrix = tfidf.fit_transform(data['review_clean_ss'])\n",
        "\n",
        "#Output the shape of tfidf_matrix\n",
        "tfidf_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqQo2yQRlKre"
      },
      "outputs": [],
      "source": [
        "#Array mapping from feature integer indices to feature name.\n",
        "tfidf.get_feature_names()[5000:5010]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TdWpaPNlPyV"
      },
      "outputs": [],
      "source": [
        "# Import linear_kernel\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "# Compute the cosine similarity matrix\n",
        "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEtFNhCNlT-O"
      },
      "outputs": [],
      "source": [
        "cosine_sim.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NS1tN_0HlWAF"
      },
      "outputs": [],
      "source": [
        "cosine_sim[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SSYw_FJB6R7"
      },
      "source": [
        "**Modelling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9MLeK0FB8Ox"
      },
      "outputs": [],
      "source": [
        "# Defining Features and splitting the data as train and test set\n",
        "\n",
        "features = data[['condition', 'usefulcount', 'sentiment', 'day', 'month', 'year',\n",
        "                   'sentiment_clean_ss', 'count_word', 'count_unique_word', 'count_letters',\n",
        "                   'count_punctuations', 'count_words_upper', 'count_words_title',\n",
        "                   'count_stopwords', 'mean_word_len']]\n",
        "\n",
        "target = data['sentiment_rate']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.25, random_state = 42)\n",
        "print (\"The Train set size \", X_train.shape)\n",
        "print (\"The Test set size \", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffgQCQPeCFie"
      },
      "source": [
        "Model - I LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsRWLVX8CIRK"
      },
      "outputs": [],
      "source": [
        "# Training Model - I\n",
        "\n",
        "clf = LGBMClassifier(\n",
        "        n_estimators=10000,\n",
        "        learning_rate=0.10,\n",
        "        num_leaves=30,\n",
        "        subsample=.9,\n",
        "        max_depth=7,\n",
        "        reg_alpha=.1,\n",
        "        reg_lambda=.1,\n",
        "        min_split_gain=.01,\n",
        "        min_child_weight=2,\n",
        "        silent=-1,\n",
        "        verbose=-1,\n",
        "        )\n",
        "model = clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "predictions = model.predict(X_test)\n",
        "print (\"The Accuracy of the model on testing data is : \", accuracy_score(y_test, predictions), '\\n')\n",
        "print (\"The confusion Matrix is \\n\")\n",
        "print (confusion_matrix(y_test, predictions), '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbktaJBDu9sG"
      },
      "outputs": [],
      "source": [
        "predictions1 = model.predict(X_train)\n",
        "print (\"The Accuracy of the model on training data is : \", accuracy_score(y_train, predictions1), '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zf6mHkkWnDNb"
      },
      "outputs": [],
      "source": [
        "#print (\"**********TRAINING SCORE********** : \\n\"classification_report(y_train, predictions),'\\n')\n",
        "#print (\"**********TESTING SCORE********** : \\n\"classification_report(y_test, predictions))\n",
        "print (classification_report(y_test, predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mG-3kAVsCSZv"
      },
      "outputs": [],
      "source": [
        "# Feature Importance Plot using LGBM\n",
        "plt.rcParams['figure.figsize'] = [12, 9]\n",
        "sns.set(style = 'darkgrid', font_scale = 1.2)\n",
        "plot_importance(model);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLbivTYW0uhr"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "auc = metrics.roc_auc_score(y_test, predictions)\n",
        "#roc_auc_score(y_score=np_pred, y_true=np_label, multi_class=\"ovr\",average=None)\n",
        "\n",
        "false_positive_rate, true_positive_rate, thresolds = metrics.roc_curve(y_test, predictions)\n",
        "\n",
        "plt.figure(figsize=(10, 8), dpi=100)\n",
        "plt.axis('scaled')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.title(\"AUC & ROC Curve\")\n",
        "plt.plot(false_positive_rate, true_positive_rate, 'g')\n",
        "plt.fill_between(false_positive_rate, true_positive_rate, facecolor='lightgreen', alpha=0.7)\n",
        "plt.text(0.95, 0.05, 'AUC = %0.4f' % auc, ha='right', fontsize=12, weight='bold', color='blue')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2iftCan7At9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "roc_auc = roc_auc_score(y_test,predictions, multi_class='ovr')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irWz25lt8YUv"
      },
      "source": [
        "Model-4 Support Vector Machine (SVM)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTXBOCftv3XJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nMchkGPwLCX"
      },
      "outputs": [],
      "source": [
        "model_svc=SVC()\n",
        "model_svm=model_svc.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSMZ33poxx1k"
      },
      "outputs": [],
      "source": [
        "# Predictions\n",
        "predictions_4 = model_svm.predict(X_test)\n",
        "print (\"The Accuracy of the model on testing data is : \", accuracy_score(y_test, predictions_4), '\\n')\n",
        "predictions_41 = model_svm.predict(X_train)\n",
        "print (\"The Accuracy of the model on training data is : \", accuracy_score(y_train, predictions_41), '\\n')\n",
        "print (\"The confusion Matrix is \\n\")\n",
        "print (confusion_matrix(y_test, predictions_4), '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ici3eBAxg0O"
      },
      "outputs": [],
      "source": [
        "predictions_41 = model_svm.predict(X_train)\n",
        "print (\"The Accuracy of the model on training data is : \", accuracy_score(y_train, predictions_4), '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhEV_-L98NwR"
      },
      "source": [
        "Model-V LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgnnnPyv7pXC"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "LR = LogisticRegression(solver=\"liblinear\").fit(X_train, y_train)\n",
        "predictions_5 = LR.predict(X_test)\n",
        "roc_auc = roc_auc_score(y_test,preds, multi_class='ovr')\n",
        "print(roc_auc) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWZb-QLUlbJP"
      },
      "source": [
        "Model - II XGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9T3m08f8lSMJ"
      },
      "outputs": [],
      "source": [
        "# Training Model - II\n",
        "\n",
        "xgb_clf = XGBClassifier(n_estimator = 10000,\n",
        "                    learning_rate=0.10,\n",
        "                    num_leaves=30)\n",
        "\n",
        "model_xgb = xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "predictions_2 = model_xgb.predict(X_test)\n",
        "print (\"The Accuracy of the model on testing data is : \", accuracy_score(y_test, predictions_2), '\\n')\n",
        "predictions_21 = model_xgb.predict(X_train)\n",
        "print (\"The Accuracy of the model on training data is : \", accuracy_score(y_train, predictions_21), '\\n')\n",
        "print (\"The confusion Matrix is \\n\")\n",
        "print (confusion_matrix(y_test, predictions_2), '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prOqB9uWnSMt"
      },
      "outputs": [],
      "source": [
        "#print (\"**********TRAINING SCORE********** : '\\n'\"classification_report(y_train, predictions_2),'\\n')\n",
        "#print (\"**********TESTING SCORE********** : '\\n'\"classification_report(y_test, predictions_2))\n",
        "print (classification_report(y_test, predictions_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVRR9JO2lSSf"
      },
      "outputs": [],
      "source": [
        "# Feature Importance Plot using XGBClassifier\n",
        "from xgboost import plot_importance # plot_importance for xgboost\n",
        "plt.rcParams['figure.figsize'] = [12, 9]\n",
        "plot_importance(model_xgb);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu0fXoCKmKdP"
      },
      "source": [
        "Model - III CatBoostClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFh3U6gDmItW"
      },
      "outputs": [],
      "source": [
        "# Training Model - III\n",
        "\n",
        "cat_clf = CatBoostClassifier(iterations = 100,\n",
        "                            learning_rate = 0.5);\n",
        "\n",
        "model_cat = cat_clf.fit(X_train, y_train);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ir9k36BomI_j"
      },
      "outputs": [],
      "source": [
        "# Predictions\n",
        "predictions_3 = model_cat.predict(X_test)\n",
        "print (\"The Accuracy of the model on testing data is : \", accuracy_score(y_test, predictions_3), '\\n')\n",
        "\n",
        "predictions_31 = model_cat.predict(X_train)\n",
        "print (\"The Accuracy of the model on training data is : \", accuracy_score(y_train, predictions_31), '\\n')\n",
        "print (\"The confusion Matrix is \\n\")\n",
        "print (confusion_matrix(y_test, predictions_3), '\\n')\n",
        "\n",
        "print (classification_report(y_test, predictions_3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teNEk0FGnhID"
      },
      "outputs": [],
      "source": [
        "print (\"**********TRAINING SCORE********** : '\\n'\"classification_report(y_test, predictions),'\\n')\n",
        "print (\"**********TESTING SCORE********** : '\\n'\"classification_report(y_test, predictions_3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n-7SMxQfSPd"
      },
      "source": [
        " K-Nearest Neighbors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nv46bocleUFB"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier = KNeighborsClassifier(n_neighbors=1000)\n",
        "classifier.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UNFSsiRgAMW"
      },
      "outputs": [],
      "source": [
        "predictions_4 = classifier.predict(X_test)\n",
        "print (\"The Accuracy of the model is : \", accuracy_score(y_test, predictions_4), '\\n')\n",
        "\n",
        "print (\"The confusion matrix of train data is: \"confusion_matrix(y_train, predictions_4), '\\n')\n",
        "print (\"The confusion matrix of test data is: \"confusion_matrix(y_test, predictions_4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yol-N8QagR1u"
      },
      "outputs": [],
      "source": [
        "print (\"**********TRAINING SCORE********** : '\\n'\"classification_report(y_train, predictions_4),'\\n')\n",
        "print (\"**********TESTING SCORE********** : '\\n'\"classification_report(y_test, predictions_4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FJg5oHlhlZK"
      },
      "source": [
        "Support Vector Machine (SVM)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIqSKHiFhkbq"
      },
      "outputs": [],
      "source": [
        "#Import svm model\n",
        "from sklearn import svm\n",
        "\n",
        "#Create a svm Classifier\n",
        "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
        "\n",
        "#Train the model using the training sets\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "predictions_5 = clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJX9zbwAiMvT"
      },
      "outputs": [],
      "source": [
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import metrics\n",
        "\n",
        "# Model Accuracy: how often is the classifier correct?\n",
        "print(\"The Accuracy of the model is:\",metrics.accuracy_score(y_test, predictions_5),'\\n')\n",
        "print (confusion_matrix(y_train, predictions_5), '\\n')\n",
        "print (confusion_matrix(y_test, predictions_5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCAJWzNYiy_8"
      },
      "outputs": [],
      "source": [
        "print (classification_report(y_train, predictions_5),'\\n')\n",
        "print (classification_report(y_test, predictions_5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yFnD7uOXBw_"
      },
      "source": [
        "Support Vector Machine (SVM)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5ETLojP6KtB"
      },
      "outputs": [],
      "source": [
        "X_train = np.nan_to_num(X_train)\n",
        "\n",
        "# import SVC classifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "# import metrics to compute accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# instantiate classifier with default hyperparameters\n",
        "svc=SVC() \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# fit classifier to training set\n",
        "svc.fit(X_train,y_train)\n",
        "\n",
        "\n",
        "# make predictions on test set\n",
        "y_pred=svc.predict(X_test)\n",
        "\n",
        "\n",
        "# compute and print accuracy score\n",
        "print('Model accuracy score with default hyperparameters: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQzSkZwBWLS_"
      },
      "outputs": [],
      "source": [
        "print (confusion_matrix(y_test, y_pred), '\\n')\n",
        "\n",
        "print (classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkouCnmagTv_"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "model=MultinomialNB()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmnK8P4zggwl"
      },
      "outputs": [],
      "source": [
        "model.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cp3FldT-jarH"
      },
      "outputs": [],
      "source": [
        "(data.values < 0).any()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnBOfwB_FoOf"
      },
      "source": [
        "Model-5 Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_Gll_tHFnR9"
      },
      "outputs": [],
      "source": [
        "# train a Gaussian Naive Bayes classifier on the training set\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "\n",
        "# instantiate the model\n",
        "gnb = GaussianNB()\n",
        "\n",
        "\n",
        "# fit the model\n",
        "model_gnb=gnb.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wgW18aWFuLQ"
      },
      "outputs": [],
      "source": [
        "predictions_5 = model_gnb.predict(X_test)\n",
        "print (\"The Accuracy of the testing model is : \", accuracy_score(y_test, predictions_5), '\\n')\n",
        "predictions_51 = model_gnb.predict(X_train)\n",
        "print (\"The Accuracy of the model on training data is : \", accuracy_score(y_train, predictions_51), '\\n')\n",
        "print (\"The confusion Matrix is \\n\")\n",
        "print (confusion_matrix(y_test, predictions_5), '\\n')\n",
        "\n",
        "print (classification_report(y_test, predictions_5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d24AdhAF9F0"
      },
      "outputs": [],
      "source": [
        "print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, predictions_5)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}